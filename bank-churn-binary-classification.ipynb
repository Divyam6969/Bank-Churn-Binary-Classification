{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/divyam6969/bank-churn-binary-classification?scriptVersionId=159353958\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Bank Churn Binary Classification\nDone by\nDivyam \n[Github](https://github.com/divyam6969)\n\n#### This project involves building a predictive model using XGBoost to identify potential customer churn in a financial dataset. The process includes preprocessing steps like handling missing values and outliers, as well as one-hot encoding categorical variables. The trained model is evaluated on a test set, and predictions are saved to a CSV file. The project output provides a detailed breakdown of churn predictions by customer segments, offering insights into the model's performance.\n\n#### For users interested in replicating or extending the project, the README file serves as a concise guide, detailing project structure, dependencies, and usage instructions. Overall, this work contributes to customer relationship management by employing machine learning to proactively detect and address potential churn in a financial context.","metadata":{}},{"cell_type":"markdown","source":"## Reading the data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-17T12:40:54.941846Z","iopub.execute_input":"2024-01-17T12:40:54.942531Z","iopub.status.idle":"2024-01-17T12:40:55.312105Z","shell.execute_reply.started":"2024-01-17T12:40:54.942493Z","shell.execute_reply":"2024-01-17T12:40:55.311399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pandas","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:40:55.31376Z","iopub.execute_input":"2024-01-17T12:40:55.314356Z","iopub.status.idle":"2024-01-17T12:41:06.163702Z","shell.execute_reply.started":"2024-01-17T12:40:55.314329Z","shell.execute_reply":"2024-01-17T12:41:06.162419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.165315Z","iopub.execute_input":"2024-01-17T12:41:06.165709Z","iopub.status.idle":"2024-01-17T12:41:06.169944Z","shell.execute_reply.started":"2024-01-17T12:41:06.165647Z","shell.execute_reply":"2024-01-17T12:41:06.169348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/playground-series-s4e1/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e1/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.171503Z","iopub.execute_input":"2024-01-17T12:41:06.171933Z","iopub.status.idle":"2024-01-17T12:41:06.754755Z","shell.execute_reply.started":"2024-01-17T12:41:06.171899Z","shell.execute_reply":"2024-01-17T12:41:06.753811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.75764Z","iopub.execute_input":"2024-01-17T12:41:06.757941Z","iopub.status.idle":"2024-01-17T12:41:06.785338Z","shell.execute_reply.started":"2024-01-17T12:41:06.757918Z","shell.execute_reply":"2024-01-17T12:41:06.784207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.78629Z","iopub.execute_input":"2024-01-17T12:41:06.78649Z","iopub.status.idle":"2024-01-17T12:41:06.801187Z","shell.execute_reply.started":"2024-01-17T12:41:06.78647Z","shell.execute_reply":"2024-01-17T12:41:06.800157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.802891Z","iopub.execute_input":"2024-01-17T12:41:06.803298Z","iopub.status.idle":"2024-01-17T12:41:06.886735Z","shell.execute_reply.started":"2024-01-17T12:41:06.803262Z","shell.execute_reply":"2024-01-17T12:41:06.885609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now we have read the data now we will explore it","metadata":{}},{"cell_type":"markdown","source":"## Exploring / Pre Processing Data","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.888363Z","iopub.execute_input":"2024-01-17T12:41:06.888661Z","iopub.status.idle":"2024-01-17T12:41:06.924704Z","shell.execute_reply.started":"2024-01-17T12:41:06.888628Z","shell.execute_reply":"2024-01-17T12:41:06.923882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### removing negative values (in Age and Balance now)","metadata":{}},{"cell_type":"code","source":"# Assuming 'Balance' and 'Age' are the columns where negative values should be removed\ntrain_data = train_data[(train_data['Balance'] >= 0) & (train_data['Age'] >= 0)]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.926081Z","iopub.execute_input":"2024-01-17T12:41:06.926586Z","iopub.status.idle":"2024-01-17T12:41:06.935203Z","shell.execute_reply.started":"2024-01-17T12:41:06.926558Z","shell.execute_reply":"2024-01-17T12:41:06.934414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:06.936611Z","iopub.execute_input":"2024-01-17T12:41:06.937211Z","iopub.status.idle":"2024-01-17T12:41:07.013393Z","shell.execute_reply.started":"2024-01-17T12:41:06.937183Z","shell.execute_reply":"2024-01-17T12:41:07.012414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### removing the outliers","metadata":{}},{"cell_type":"code","source":"# Calculate IQR excluding the last column (\"Exited\")\nQ1 = train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).quantile(0.25)\nQ3 = train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).quantile(0.75)\nIQR = Q3 - Q1\n\n# Identify outliers\nlower_fence = Q1 - 1.5 * IQR\nupper_fence = Q3 + 1.5 * IQR\noutliers = (\n    (train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']) < lower_fence) | \n    (train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']) > upper_fence)\n).any(axis=1)\n\n# Remove outliers\ntrain_data = train_data[~outliers]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:07.014554Z","iopub.execute_input":"2024-01-17T12:41:07.015124Z","iopub.status.idle":"2024-01-17T12:41:07.146666Z","shell.execute_reply.started":"2024-01-17T12:41:07.015094Z","shell.execute_reply":"2024-01-17T12:41:07.145749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:07.147954Z","iopub.execute_input":"2024-01-17T12:41:07.148206Z","iopub.status.idle":"2024-01-17T12:41:07.210667Z","shell.execute_reply.started":"2024-01-17T12:41:07.148184Z","shell.execute_reply":"2024-01-17T12:41:07.209696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### now i would replace the null values with the mean","metadata":{}},{"cell_type":"code","source":"# Extract numeric columns excluding the last column (\"Exited\")\nnumeric_columns = train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).columns\n\n# Calculate column-wise means excluding the last column (\"Exited\")\ncolumn_means = train_data.iloc[:, :-1][numeric_columns].mean()\n\n# Create a copy of the selected columns and fill null values with the corresponding mean\ntrain_data[numeric_columns] = train_data[numeric_columns].copy().fillna(column_means)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:07.211765Z","iopub.execute_input":"2024-01-17T12:41:07.211997Z","iopub.status.idle":"2024-01-17T12:41:07.241864Z","shell.execute_reply.started":"2024-01-17T12:41:07.211975Z","shell.execute_reply":"2024-01-17T12:41:07.241018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now removing the null values for non numeric columns","metadata":{}},{"cell_type":"code","source":"train_data.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:07.245954Z","iopub.execute_input":"2024-01-17T12:41:07.246208Z","iopub.status.idle":"2024-01-17T12:41:07.272291Z","shell.execute_reply.started":"2024-01-17T12:41:07.246187Z","shell.execute_reply":"2024-01-17T12:41:07.270975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:07.273613Z","iopub.execute_input":"2024-01-17T12:41:07.274002Z","iopub.status.idle":"2024-01-17T12:41:07.333419Z","shell.execute_reply.started":"2024-01-17T12:41:07.273972Z","shell.execute_reply":"2024-01-17T12:41:07.33257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:07.334543Z","iopub.execute_input":"2024-01-17T12:41:07.3348Z","iopub.status.idle":"2024-01-17T12:41:07.358085Z","shell.execute_reply.started":"2024-01-17T12:41:07.334778Z","shell.execute_reply":"2024-01-17T12:41:07.357046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have removed the faaltu ki values and our data is preprocessed now, now we would train the data and test it on test_csv file","metadata":{}},{"cell_type":"markdown","source":"#### Plotting the data to see if there's any outlier","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\n# Assuming train_data is your cleaned DataFrame\n# ...\n\n# Assuming continuous_features and categorical_features are lists of column names\ncontinuous_features = train_data.select_dtypes(include=['float64', 'int64']).columns\ncategorical_features = ['Geography', 'Gender']  # Update with your actual categorical columns\n\n# Set up the subplots\nnum_cols_continuous = len(continuous_features)\nnum_cols_categorical = len(categorical_features)\n\nnum_rows_continuous = math.ceil(num_cols_continuous / 2)\nnum_rows_categorical = math.ceil(num_cols_categorical / 2)\n\n# Set up the subplots for continuous features\nfig, ax = plt.subplots(num_rows_continuous, 2, figsize=(16, 5 * num_rows_continuous))\n\n# Choose a suitable color palette\nbox_color_palette = sns.color_palette(\"pastel\")\n\nfor i, col in enumerate(continuous_features):\n    # Create a boxplot using sns.boxplot with styling\n    sns.boxplot(data=train_data, x=col, color=box_color_palette[i % len(box_color_palette)], ax=ax.flatten()[i])\n    \n    ax.flatten()[i].set_title(f'Boxplot of {str(col).upper()}', fontsize=14)\n    ax.flatten()[i].set_xlabel(col, fontsize=12)\n    ax.flatten()[i].set_ylabel('Value', fontsize=12)\n    \n    # Add grid for better readability\n    ax.flatten()[i].grid(axis='y', linestyle='--', alpha=0.7)\n\n# Hide any empty subplots\nfor j in range(i + 1, num_rows_continuous * 2):\n    fig.delaxes(ax.flatten()[j])\n\n# Set up the subplots for categorical features\nfig, ax = plt.subplots(num_rows_categorical, 2, figsize=(16, 5 * num_rows_categorical))\n\n# Choose a suitable color palette\ncountplot_color_palette = sns.color_palette(\"muted\")\n\nfor i, col in enumerate(categorical_features):\n    # Create a countplot using sns.countplot with styling\n    sns.countplot(x=col, data=train_data, palette=countplot_color_palette, ax=ax.flatten()[i])\n    \n    ax.flatten()[i].set_title(f'Countplot of {str(col).upper()}', fontsize=14)\n    ax.flatten()[i].set_xlabel(col, fontsize=12)\n    ax.flatten()[i].set_ylabel('Count', fontsize=12)\n    \n    # Add grid for better readability\n    ax.flatten()[i].grid(axis='y', linestyle='--', alpha=0.7)\n\n# Hide any empty subplots\nfor j in range(i + 1, num_rows_categorical * 2):\n    fig.delaxes(ax.flatten()[j])\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:07.359705Z","iopub.execute_input":"2024-01-17T12:41:07.360175Z","iopub.status.idle":"2024-01-17T12:41:09.587628Z","shell.execute_reply.started":"2024-01-17T12:41:07.360141Z","shell.execute_reply":"2024-01-17T12:41:09.586749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we would train the data using different models and choose the model whose accuracy is the best and test the data on train_csv","metadata":{}},{"cell_type":"markdown","source":"## Data Modelling","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:09.588613Z","iopub.execute_input":"2024-01-17T12:41:09.589383Z","iopub.status.idle":"2024-01-17T12:41:09.604721Z","shell.execute_reply.started":"2024-01-17T12:41:09.589355Z","shell.execute_reply":"2024-01-17T12:41:09.603574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:09.606111Z","iopub.execute_input":"2024-01-17T12:41:09.60647Z","iopub.status.idle":"2024-01-17T12:41:09.625757Z","shell.execute_reply.started":"2024-01-17T12:41:09.60644Z","shell.execute_reply":"2024-01-17T12:41:09.624931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.describe())","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:09.626784Z","iopub.execute_input":"2024-01-17T12:41:09.627276Z","iopub.status.idle":"2024-01-17T12:41:09.682478Z","shell.execute_reply.started":"2024-01-17T12:41:09.627252Z","shell.execute_reply":"2024-01-17T12:41:09.681652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data.describe())","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:09.683588Z","iopub.execute_input":"2024-01-17T12:41:09.684332Z","iopub.status.idle":"2024-01-17T12:41:09.733813Z","shell.execute_reply.started":"2024-01-17T12:41:09.684301Z","shell.execute_reply":"2024-01-17T12:41:09.732746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Separate features (X) and target variable (y)\nX = train_data.drop(columns=['Exited'])  # Exclude the target variable\ny = train_data['Exited']\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:09.73547Z","iopub.execute_input":"2024-01-17T12:41:09.73577Z","iopub.status.idle":"2024-01-17T12:41:10.05447Z","shell.execute_reply.started":"2024-01-17T12:41:09.735744Z","shell.execute_reply":"2024-01-17T12:41:10.053056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training the model using XGBClassfier Model","metadata":{}},{"cell_type":"code","source":"\n# Define categorical and numeric features\ncategorical_features = ['Surname', 'Geography', 'Gender']\nnumeric_features = [col for col in X.columns if col not in categorical_features]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:10.055781Z","iopub.execute_input":"2024-01-17T12:41:10.056111Z","iopub.status.idle":"2024-01-17T12:41:10.061204Z","shell.execute_reply.started":"2024-01-17T12:41:10.056083Z","shell.execute_reply":"2024-01-17T12:41:10.05988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create a column transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:10.062975Z","iopub.execute_input":"2024-01-17T12:41:10.063342Z","iopub.status.idle":"2024-01-17T12:41:10.072252Z","shell.execute_reply.started":"2024-01-17T12:41:10.063314Z","shell.execute_reply":"2024-01-17T12:41:10.071419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Combine the preprocessor with the classifier in a pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('classifier', XGBClassifier())])\n\n# Split the data into training and testing sets\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming 'y' is a continuous variable\nthreshold = 0.5  # set your desired threshold\n\n# Ensure the indices are aligned\ny_binary = (y_train > threshold).astype(int)\n\n# Train the model\npipeline.fit(X_train, y_binary)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:41:10.073442Z","iopub.execute_input":"2024-01-17T12:41:10.073723Z","iopub.status.idle":"2024-01-17T12:41:11.07712Z","shell.execute_reply.started":"2024-01-17T12:41:10.073665Z","shell.execute_reply":"2024-01-17T12:41:11.076255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = pipeline.predict(test_data)\ntrain_predictions = pipeline.predict(X_train)  # Assuming X_train is the feature matrix used for training\n\n# Assuming 'y_train' is the target variable used for training\ntrain_accuracy = accuracy_score(y_train, train_predictions)\nprint(f'Training Accuracy: {train_accuracy}')\n# Save the predictions to a CSV file\npredictions_df = pd.DataFrame({'id': test_data['id'], 'Exited': test_predictions})\npredictions_df.to_csv('DivyamKaAnswer.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:46:37.367932Z","iopub.execute_input":"2024-01-17T12:46:37.368278Z","iopub.status.idle":"2024-01-17T12:46:38.079019Z","shell.execute_reply.started":"2024-01-17T12:46:37.368251Z","shell.execute_reply":"2024-01-17T12:46:38.078213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub[\"id\"] = test_data['id']\nsub[\"Exited\"] = pipeline.predict_proba(test_data)[:,1]\n\nsub.to_csv(\"submission.csv\",header=True,index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:55:05.771339Z","iopub.execute_input":"2024-01-17T12:55:05.772578Z","iopub.status.idle":"2024-01-17T12:55:06.289118Z","shell.execute_reply.started":"2024-01-17T12:55:05.772507Z","shell.execute_reply":"2024-01-17T12:55:06.288255Z"},"trusted":true},"execution_count":null,"outputs":[]}]}