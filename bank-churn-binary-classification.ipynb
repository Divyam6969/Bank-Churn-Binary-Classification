{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/divyam6969/bank-churn-binary-classification?scriptVersionId=159312804\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Bank Churn Binary Classification\nDone by\nDivyam \n[Github](http://https://github.com/divyam6969)\n\n#### This project involves building a predictive model using XGBoost to identify potential customer churn in a financial dataset. The process includes preprocessing steps like handling missing values and outliers, as well as one-hot encoding categorical variables. The trained model is evaluated on a test set, and predictions are saved to a CSV file. The project output provides a detailed breakdown of churn predictions by customer segments, offering insights into the model's performance.\n\n#### For users interested in replicating or extending the project, the README file serves as a concise guide, detailing project structure, dependencies, and usage instructions. Overall, this work contributes to customer relationship management by employing machine learning to proactively detect and address potential churn in a financial context.","metadata":{}},{"cell_type":"markdown","source":"## Reading the data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-17T09:19:15.785329Z","iopub.execute_input":"2024-01-17T09:19:15.785688Z","iopub.status.idle":"2024-01-17T09:19:15.793071Z","shell.execute_reply.started":"2024-01-17T09:19:15.78566Z","shell.execute_reply":"2024-01-17T09:19:15.791961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pandas","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:47:18.176326Z","iopub.execute_input":"2024-01-17T10:47:18.177368Z","iopub.status.idle":"2024-01-17T10:47:30.208179Z","shell.execute_reply.started":"2024-01-17T10:47:18.177322Z","shell.execute_reply":"2024-01-17T10:47:30.206567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:47:32.636683Z","iopub.execute_input":"2024-01-17T10:47:32.637126Z","iopub.status.idle":"2024-01-17T10:47:32.643624Z","shell.execute_reply.started":"2024-01-17T10:47:32.637083Z","shell.execute_reply":"2024-01-17T10:47:32.642414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/playground-series-s4e1/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e1/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:50:56.070174Z","iopub.execute_input":"2024-01-17T10:50:56.070714Z","iopub.status.idle":"2024-01-17T10:50:56.459496Z","shell.execute_reply.started":"2024-01-17T10:50:56.070673Z","shell.execute_reply":"2024-01-17T10:50:56.458257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:47:45.384764Z","iopub.execute_input":"2024-01-17T10:47:45.385415Z","iopub.status.idle":"2024-01-17T10:47:45.410347Z","shell.execute_reply.started":"2024-01-17T10:47:45.385368Z","shell.execute_reply":"2024-01-17T10:47:45.408578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:20:50.319134Z","iopub.execute_input":"2024-01-17T09:20:50.319542Z","iopub.status.idle":"2024-01-17T09:20:50.338535Z","shell.execute_reply.started":"2024-01-17T09:20:50.319506Z","shell.execute_reply":"2024-01-17T09:20:50.337337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:48:02.275832Z","iopub.execute_input":"2024-01-17T10:48:02.276792Z","iopub.status.idle":"2024-01-17T10:48:02.384174Z","shell.execute_reply.started":"2024-01-17T10:48:02.276754Z","shell.execute_reply":"2024-01-17T10:48:02.383071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now we have read the data now we will explore it","metadata":{}},{"cell_type":"markdown","source":"## Exploring / Pre Processing Data","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:48:09.839956Z","iopub.execute_input":"2024-01-17T10:48:09.840412Z","iopub.status.idle":"2024-01-17T10:48:09.883763Z","shell.execute_reply.started":"2024-01-17T10:48:09.840378Z","shell.execute_reply":"2024-01-17T10:48:09.882766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### removing negative values (in Age and Balance now)","metadata":{}},{"cell_type":"code","source":"# Assuming 'Balance' and 'Age' are the columns where negative values should be removed\ntrain_data = train_data[(train_data['Balance'] >= 0) & (train_data['Age'] >= 0)]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:51:03.190905Z","iopub.execute_input":"2024-01-17T10:51:03.192471Z","iopub.status.idle":"2024-01-17T10:51:03.209545Z","shell.execute_reply.started":"2024-01-17T10:51:03.19241Z","shell.execute_reply":"2024-01-17T10:51:03.207757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:48:36.411821Z","iopub.execute_input":"2024-01-17T10:48:36.412739Z","iopub.status.idle":"2024-01-17T10:48:36.519582Z","shell.execute_reply.started":"2024-01-17T10:48:36.41269Z","shell.execute_reply":"2024-01-17T10:48:36.518383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### removing the outliers","metadata":{}},{"cell_type":"code","source":"# Calculate IQR excluding the last column (\"Exited\")\nQ1 = train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).quantile(0.25)\nQ3 = train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).quantile(0.75)\nIQR = Q3 - Q1\n\n# Identify outliers\nlower_fence = Q1 - 1.5 * IQR\nupper_fence = Q3 + 1.5 * IQR\noutliers = (\n    (train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']) < lower_fence) | \n    (train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']) > upper_fence)\n).any(axis=1)\n\n# Remove outliers\ntrain_data = train_data[~outliers]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:51:07.340619Z","iopub.execute_input":"2024-01-17T10:51:07.341543Z","iopub.status.idle":"2024-01-17T10:51:07.491491Z","shell.execute_reply.started":"2024-01-17T10:51:07.3415Z","shell.execute_reply":"2024-01-17T10:51:07.490287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:51:14.29244Z","iopub.execute_input":"2024-01-17T10:51:14.293763Z","iopub.status.idle":"2024-01-17T10:51:14.381317Z","shell.execute_reply.started":"2024-01-17T10:51:14.293705Z","shell.execute_reply":"2024-01-17T10:51:14.380265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### now i would replace the null values with the mean","metadata":{}},{"cell_type":"code","source":"# Extract numeric columns excluding the last column (\"Exited\")\nnumeric_columns = train_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).columns\n\n# Calculate column-wise means excluding the last column (\"Exited\")\ncolumn_means = train_data.iloc[:, :-1][numeric_columns].mean()\n\n# Create a copy of the selected columns and fill null values with the corresponding mean\ntrain_data[numeric_columns] = train_data[numeric_columns].copy().fillna(column_means)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:52:48.350434Z","iopub.execute_input":"2024-01-17T10:52:48.350884Z","iopub.status.idle":"2024-01-17T10:52:48.398596Z","shell.execute_reply.started":"2024-01-17T10:52:48.350852Z","shell.execute_reply":"2024-01-17T10:52:48.397304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now removing the null values for non numeric columns","metadata":{}},{"cell_type":"code","source":"train_data.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:52:58.334055Z","iopub.execute_input":"2024-01-17T10:52:58.334481Z","iopub.status.idle":"2024-01-17T10:52:58.368561Z","shell.execute_reply.started":"2024-01-17T10:52:58.334441Z","shell.execute_reply":"2024-01-17T10:52:58.367371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:53:09.883148Z","iopub.execute_input":"2024-01-17T10:53:09.883577Z","iopub.status.idle":"2024-01-17T10:53:09.970069Z","shell.execute_reply.started":"2024-01-17T10:53:09.883544Z","shell.execute_reply":"2024-01-17T10:53:09.96892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:53:38.845861Z","iopub.execute_input":"2024-01-17T10:53:38.846345Z","iopub.status.idle":"2024-01-17T10:53:38.877581Z","shell.execute_reply.started":"2024-01-17T10:53:38.846309Z","shell.execute_reply":"2024-01-17T10:53:38.876434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have removed the faaltu ki values and our data is preprocessed now, now we would train the data and test it on test_csv file","metadata":{}},{"cell_type":"markdown","source":"#### Plotting the data to see if there's any outlier","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\n# Assuming train_data is your cleaned DataFrame\n# ...\n\n# Assuming continuous_features and categorical_features are lists of column names\ncontinuous_features = train_data.select_dtypes(include=['float64', 'int64']).columns\ncategorical_features = ['Geography', 'Gender']  # Update with your actual categorical columns\n\n# Set up the subplots\nnum_cols_continuous = len(continuous_features)\nnum_cols_categorical = len(categorical_features)\n\nnum_rows_continuous = math.ceil(num_cols_continuous / 2)\nnum_rows_categorical = math.ceil(num_cols_categorical / 2)\n\n# Set up the subplots for continuous features\nfig, ax = plt.subplots(num_rows_continuous, 2, figsize=(16, 5 * num_rows_continuous))\n\n# Choose a suitable color palette\nbox_color_palette = sns.color_palette(\"pastel\")\n\nfor i, col in enumerate(continuous_features):\n    # Create a boxplot using sns.boxplot with styling\n    sns.boxplot(data=train_data, x=col, color=box_color_palette[i % len(box_color_palette)], ax=ax.flatten()[i])\n    \n    ax.flatten()[i].set_title(f'Boxplot of {str(col).upper()}', fontsize=14)\n    ax.flatten()[i].set_xlabel(col, fontsize=12)\n    ax.flatten()[i].set_ylabel('Value', fontsize=12)\n    \n    # Add grid for better readability\n    ax.flatten()[i].grid(axis='y', linestyle='--', alpha=0.7)\n\n# Hide any empty subplots\nfor j in range(i + 1, num_rows_continuous * 2):\n    fig.delaxes(ax.flatten()[j])\n\n# Set up the subplots for categorical features\nfig, ax = plt.subplots(num_rows_categorical, 2, figsize=(16, 5 * num_rows_categorical))\n\n# Choose a suitable color palette\ncountplot_color_palette = sns.color_palette(\"muted\")\n\nfor i, col in enumerate(categorical_features):\n    # Create a countplot using sns.countplot with styling\n    sns.countplot(x=col, data=train_data, palette=countplot_color_palette, ax=ax.flatten()[i])\n    \n    ax.flatten()[i].set_title(f'Countplot of {str(col).upper()}', fontsize=14)\n    ax.flatten()[i].set_xlabel(col, fontsize=12)\n    ax.flatten()[i].set_ylabel('Count', fontsize=12)\n    \n    # Add grid for better readability\n    ax.flatten()[i].grid(axis='y', linestyle='--', alpha=0.7)\n\n# Hide any empty subplots\nfor j in range(i + 1, num_rows_categorical * 2):\n    fig.delaxes(ax.flatten()[j])\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:53:43.518899Z","iopub.execute_input":"2024-01-17T10:53:43.519319Z","iopub.status.idle":"2024-01-17T10:53:45.78229Z","shell.execute_reply.started":"2024-01-17T10:53:43.519285Z","shell.execute_reply":"2024-01-17T10:53:45.781096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we would train the data using different models and choose the model whose accuracy is the best and test the data on train_csv","metadata":{}},{"cell_type":"markdown","source":"## Data Modelling","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:54:19.99469Z","iopub.execute_input":"2024-01-17T10:54:19.995124Z","iopub.status.idle":"2024-01-17T10:54:20.015459Z","shell.execute_reply.started":"2024-01-17T10:54:19.995092Z","shell.execute_reply":"2024-01-17T10:54:20.014071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:54:24.036361Z","iopub.execute_input":"2024-01-17T10:54:24.036743Z","iopub.status.idle":"2024-01-17T10:54:24.055466Z","shell.execute_reply.started":"2024-01-17T10:54:24.036713Z","shell.execute_reply":"2024-01-17T10:54:24.054212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.describe())","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:54:28.365145Z","iopub.execute_input":"2024-01-17T10:54:28.365545Z","iopub.status.idle":"2024-01-17T10:54:28.445173Z","shell.execute_reply.started":"2024-01-17T10:54:28.365506Z","shell.execute_reply":"2024-01-17T10:54:28.444016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data.describe())","metadata":{"execution":{"iopub.status.busy":"2024-01-17T10:54:33.320584Z","iopub.execute_input":"2024-01-17T10:54:33.32098Z","iopub.status.idle":"2024-01-17T10:54:33.388963Z","shell.execute_reply.started":"2024-01-17T10:54:33.320949Z","shell.execute_reply":"2024-01-17T10:54:33.387909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Separate features (X) and target variable (y)\nX = train_data.drop(columns=['Exited'])  # Exclude the target variable\ny = train_data['Exited']\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:02:25.537783Z","iopub.execute_input":"2024-01-17T11:02:25.538257Z","iopub.status.idle":"2024-01-17T11:02:25.556631Z","shell.execute_reply.started":"2024-01-17T11:02:25.538222Z","shell.execute_reply":"2024-01-17T11:02:25.555214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training the model using XGBClassfier Model","metadata":{}},{"cell_type":"code","source":"\n# Define categorical and numeric features\ncategorical_features = ['Surname', 'Geography', 'Gender']\nnumeric_features = [col for col in X.columns if col not in categorical_features]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:03:04.300686Z","iopub.execute_input":"2024-01-17T11:03:04.301377Z","iopub.status.idle":"2024-01-17T11:03:04.30924Z","shell.execute_reply.started":"2024-01-17T11:03:04.30132Z","shell.execute_reply":"2024-01-17T11:03:04.307774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create a column transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', numeric_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:03:22.865512Z","iopub.execute_input":"2024-01-17T11:03:22.865965Z","iopub.status.idle":"2024-01-17T11:03:22.87279Z","shell.execute_reply.started":"2024-01-17T11:03:22.86593Z","shell.execute_reply":"2024-01-17T11:03:22.871227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Combine the preprocessor with the classifier in a pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('classifier', XGBClassifier())])\n\n# Split the data into training and testing sets\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming 'y' is a continuous variable\nthreshold = 0.5  # set your desired threshold\n\n# Ensure the indices are aligned\ny_binary = (y_train > threshold).astype(int)\n\n# Train the model\npipeline.fit(X_train, y_binary)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:06:57.707412Z","iopub.execute_input":"2024-01-17T11:06:57.707829Z","iopub.status.idle":"2024-01-17T11:06:59.027906Z","shell.execute_reply.started":"2024-01-17T11:06:57.707799Z","shell.execute_reply":"2024-01-17T11:06:59.026859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = pipeline.predict(test_data)\n\ntrain_accuracy = accuracy_score(y_binary, train_predictions)\nprint(f'Training Accuracy: {train_accuracy}')\n\n# Save the predictions to a CSV file\npredictions_df = pd.DataFrame({'id': test_data['id'],'Exited': test_predictions})\npredictions_df.to_csv('DivyamKaAnswer.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:16:05.242171Z","iopub.execute_input":"2024-01-17T11:16:05.242623Z","iopub.status.idle":"2024-01-17T11:16:05.839033Z","shell.execute_reply.started":"2024-01-17T11:16:05.242588Z","shell.execute_reply":"2024-01-17T11:16:05.838007Z"},"trusted":true},"execution_count":null,"outputs":[]}]}